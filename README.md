# DLAI4
## Домашнее задание к уроку 4: Сверточные сети
### Задание 1: Сравнение CNN и полносвязных сетей
#### 1.1 Сравнение на MNIST
На этом этапе были созданы следующие три нейронные сети:
 - Полносвязная сеть (input_size, 512, 256, 128)
 - Простая CNN (input_size, (32, 64) - conv слои, feature_dims, 128)
 - CNN с Residual Block (input_size, (32, 64) - residual слои, feature_dims)

Все три модели были обучены на датасете mnist. Полные результаты обучения можно найти в файле "results/mnist_comparison/mnist_results.csv". Ниже я приведу выводы, полученные после анализа этого файла.
 - У всех моделей точность на на train превышает значение в 0.99 (ResCNN - 0.9941, CNN - 0.9909, FC - 0.9927). На test лучшая точность у ResCNN (0.9922), на втором месте CNN (0.9920) и на третьем месте FC (0.9776).
 - Проанализируем количество параметров: 

| model  | params | train_time (s) | inference time (s) |
|--------|--------|----------------|--------------------|
| FC     | 567434 | 112.75         | 1.82               |
| CNN    | 421642 | 148.85         | 1.93               |
| ResCNN | 108042 | 189.42         | 2.46               |

 - Как можно увидеть из таблицы, наибольшим количеством параметров обладает FC, при этом она обладает наименьшим временем обучения и временем предсказания. При этом модель с наименьшим количеством параметров обучается и предсказывает намного медленнее. Это можно объяснить различной сложностью операций. Например, операции свёртки у ResCNN и просто CNN намного сложнее матричных умножений у FC.

Ниже представлены кривые обучения модели CNN. Я выбрал именно её, так как, как мне кажется, она имеет лучшие кривые обучения среди других двух моделей.

![mnist_CNN_acc.png](plots%2Fmnist_comparison%2Fmnist_CNN_acc.png)

#### 1.2 Сравнение на CIFAR-10
Для этого этапа были построены и обучены на cifar10 следующие модели:
 - Полносвязная сеть (input_size, 2048, 1024, 512, 256)
 - CNN с Residual блоками (input_size, (32, 64), feature_dims)
 - CNN с регуляризацией и Residual блоками (input_size, (32, 64), feature_dims) + dropout(0.25)

Полные результаты обучения можно увидеть в файле results/cifar_comparison/cifar10_results.csv. Ниже приведены выводы, полученные после анализа этих данных.

| model     | params  | train_time (s) | best_test_acc |
|-----------|---------|----------------|---------------|
| FC        | 9050378 | 156.67         | 0.5539        |
| ResCNN    | 87498   | 516.70         | 0.7417        |
| RegResCNN | 87498   | 504.60         | 0.7262        |

 - Как можно увидеть, у моделей, использующих Residual слои точность получилось намного выше, чем у обычной полносвязной нейросети, однако время обучения у них также намного больше.

Теперь приведу графики обучения, чтобы проанализировать наличие переобучения.

FC

![cifar10_FC_acc.png](plots%2Fcifar_comparison%2Fcifar10_FC_acc.png)

Как можно увидеть, переобучение есть.

ResCNN

![cifar10_ResCNN_acc.png](plots%2Fcifar_comparison%2Fcifar10_ResCNN_acc.png)

Здесь наличие переобучения уже не так очевидно, но можно предположить, что оно есть.

RegResCNN

![cifar10_RegResCNN_acc.png](plots%2Fcifar_comparison%2Fcifar10_RegResCNN_acc.png)

Здесь переобучение отсутствует.

Ниже представлена матрица ошибок для RegResCNN.

![cifar10_RegResCNN_cm.png](plots%2Fcifar_comparison%2Fcifar10_RegResCNN_cm.png)

Теперь перейдём к анализу градиентов:
- ResCNN имеет чуть более низкую среднюю норму градиента, чем FC. Это может говорить о том, что в ResNet‑блоках градиент «сжимается» сильнее и на основные свёрточные ветви приходит чуть меньшая величина обновления.
- RegResCNN показывает большую среднюю норму градиента, чем ResCNN. Вероятно, регуляризаторы препятствуют преждевременному затуханию градиента и поддерживают его величину на более высоком уровне, что помогает модели лучше обобщать.

### Задание 2: Анализ архитектур CNN
#### 2.1 Влияние размера ядра свертки

| kernel        | dataset | train_time (s) | best_test_acc | receptive_fields |
|---------------|---------|----------------|---------------|------------------|
| Kernel_3x3    | mnist   | 84.39          | 0.4403        | 5                |
| Kernel_5x5    | mnist   | 77.96          | 0.5240        | 9                |
| Kernel_7x7    | mnist   | 79.34          | 0.4941        | 13               |
| Mixed 1x1+3x3 | mnist   | 86.53          | 0.3803        | 3                |
| Kernel_3x3    | cifar10 | 84.60          | 0.3440        | 5                |
| Kernel_5x5    | cifar10 | 79.19          | 0.3288        | 9                |
| Kernel_7x7    | cifar10 | 78.93          | 0.2121        | 13               |
| Mixed 1x1+3x3 | cifar10 | 79.20          | 0.2788        | 3                |

Выводы по скорости и точности:
 - Самые быстрые тренировки получаются у 5×5 и 7×7 ядер, тогда как у 3×3 и смешанных моделей.
 - Лучшее качество на MNIST достигает Kernel_5x5 (0.5240), затем идёт 7×7 (0.4941) и 3×3 (0.4403). Смешанная 1×1+3×3 на MNIST заметно хуже (0.3803).
 - Лучшее качество на CIFAR10 — у Kernel_3x3 (0.3440), чуть хуже идёт 5×5 (0.3288), а большие ядра (7×7) и смешанная схема показывают сильный провал (<0.28).

Выводы по рецептивным полям:
 - Умеренное RF (~9) лучше для однотонных, простых объектов (MNIST).
 - Небольшое RF (~5) лучше для сложных, мелко‑текстурных сцен (CIFAR10).
 - Слишком большое RF приводит к нехватке каналов и падению производительности.
 - Комбинация мелких ядер без увеличения охвата (mixed) даёт слишком узкий контекст.

Ниже прелставлены активации первого слоя для kernel 5x5 на mnist, так как она имеет лучшее качество. Остальные подобные карты модно найти по путям plots/cifar_architecture_analysis и plots/mnist_architecture_analysis.

![kernel_5x5_first_layer.png](plots%2Fmnist_architecture_analysis%2Fkernel_5x5_first_layer.png)

#### 2.2 Влияние глубины CNN

| model      | dataset | train_time (s) | best_test_acc |
|------------|---------|----------------|---------------|
| Shallow_2  | mnist   | 71.02          | 0.4716        |
| Medium_4   | mnist   | 84.51          | 0.7158        |
| Deep_6     | mnist   | 99.92          | 0.7746        |
| ResNetLike | mnist   | 112.60         | 0.8452        |
| Shallow_2  | cifar10 | 142.77         | 0.3510        |
| Medium_4   | cifar10 | 321.75         | 0.3642        |
| Deep_6     | cifar10 | 498.76         | 0.3569        |
| ResNetLike | cifar10 | 577.76         | 0.3904        |

Сравнение точности и времени обучения:
 - На датасете mnist более менее хорошая точность у Medium_4, Deep_6 и ResNetLike. Также можно заметить, что время обучения напрямую зависит от количества свёрточных слоёв.
 - На датасете cifar10 получить приемлимую точность так и не удалось. Насчёт времени обученияы можно сказать тоже самое, что и для датасета mnist.
 - Так же можно сделать вывод о эффективности Residual связей. Обе модели, которые их используют показали наибольшую точность, но при этом и наибольшее время обучения.

Анализ градиентов:

mnist, ResNetLike:

![ResNetLike_grad_flow.png](plots%2Fmnist_architecture_analysis%2FResNetLike_grad_flow.png)

Как можно увидеть, проблемы "взрыва" или "затухания" градиентов отсутствуют.

cifar10, ResNetLike:

![ResNetLike_grad_flow.png](plots%2Fcifar_architecture_analysis%2FResNetLike_grad_flow.png)

На этом графике также этих проблем не наблюдается.

mnist, ResNetLike, feature_map:

![ResNetLike_first_layer.png](plots%2Fmnist_architecture_analysis%2FResNetLike_first_layer.png)

cifar10, ResNetLike, feature_map:

![ResNetLike_first_layer.png](plots%2Fcifar_architecture_analysis%2FResNetLike_first_layer.png)

### Задание 3: Кастомные слои и эксперименты
#### 3.1 Реализация кастомных слоев
На данном этапе мною были реализованы следующие кастомные слои:
 - Кастомный сверточный слой с дополнительной логикой. Была добавлена кастомная логика вычисления градиентов, чтобы вручную контролировать backward-проход. Это открывает возможности проводить следующие действия и эксперименты:
   - экспериментировать с градиентами
   - внедрять нестандартные операции
   - экономить память
   - дебажить backward
 - Attention механизм для CNN. Вычисляет spatial attention map через среднее и максимум по каналам, чтобы усилить важные регионы в feature map.
 - Кастомная функция активации. NormalizedSwish — Swish-активация, у которой выходы и градиенты нормализованы делением на их максимум, чтобы ограничить масштаб значений.
 - Кастомный pooling слой. LPPooling2d выполняет пуллинг, вычисляя Lp-норму значений в каждом окне.

| layer            | dataset | custom time (s) | standard time (s) |
|------------------|---------|-----------------|-------------------|
| CustomConv2d     | mnist   | 0.1460004       | 0.0009999         |
| SpatialAttention | mnist   | 0.0309989       | 0.0010016         |
| NormalizedSwish  | mnist   | 0.0009990       | 0.0029993         |
| LPPooling2d      | mnist   | 0.0430007       | 0.0010002         |

Как можно увидеть по таблице, у кастомных слоёв время несколько хуже, чем у стандартных слоёв. Это можно объяснить усложнением логики работы кастомных слоёв. Особенно сильное увеличение времени можно увидеть у кастомного слоя CustomConv2d.
#### 3.2 Эксперименты с Residual блоками
На этом этапе мною были созданы и протестированы следующие варианты Residual блоков:
 - Базовый Residual блок
 - Bottleneck Residual блок
 - Wide Residual блок

Для тестирования всех блоков применялась одинаковая структура нейросети. Использовался датасет mnist.

| block                    | epochs | params | max acc | std acc | learn time (s) |
|--------------------------|--------|--------|---------|---------|----------------|
| Базовый Residual блок    | 10     | 4704   | 0.8853  | 0.1894  | 126.80         |
| Bottleneck Residual блок | 10     | 2960   | 0.8595  | 0.2040  | 125.84         |
| Wide Residual блок       | 10     | 812    | 0.4756  | 0.1106  | 137.66         |

- **Производительность**  
  Базовый Residual блок показал наибольшую максимальную точность (0.8853), за ним следует Bottleneck (0.8595). Wide Residual блок значительно уступает в качестве (0.4756).

- **Анализ количества параметров**  
  Bottleneck Residual блок имеет меньше всего параметров (2960) по сравнению с базовым (4704), при этом демонстрируя довольно близкую по точности производительность. Wide Residual блок — самый компактный (812 параметров), но это сказывается на точности.

- **Стабильность обучения**  
  Самая низкая дисперсия точности наблюдается у Wide Residual блока (std = 0.1106), что говорит о более стабильном, хоть и более слабом, обучении. У базового блока дисперсия 0.1894, у Bottleneck — 0.2040, то есть обучение менее предсказуемо, но даёт более высокие пики точности.